{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suvodeepmajumder/Conda/anaconda3/lib/python3.7/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#from graphviz import Digraph\n",
    "import scipy.spatial.distance\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "#Clustering birch\n",
    "from freediscovery.cluster import birch_hierarchy_wrapper\n",
    "from freediscovery.cluster import Birch,BirchSubcluster\n",
    "#Sklearn\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "#Learners\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import OneClassSVM\n",
    "#Distance measure\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcluster(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.parent = None\n",
    "        self.parent_id = None\n",
    "        self.depth = None\n",
    "        self.size = None\n",
    "        self.cluster_id = None\n",
    "        self.data_points = []\n",
    "        self.test_points = {}\n",
    "        self.test_labels = {}\n",
    "        self.predicted = {}\n",
    "        self.centroid = None\n",
    "        self.classifier = None\n",
    "        self.outlier_model = None\n",
    "        self.cluster_obj = None\n",
    "        self.outlier_points = []\n",
    "        self.score = []\n",
    "        self.d1 = None\n",
    "        self.d2 = None\n",
    "        self.threshold = None\n",
    "        self.last_retrained = 1\n",
    "        self.last_certified = 0\n",
    "    \n",
    "    def set_parent(self,parent_node=None):\n",
    "        if parent_node == None:\n",
    "            self.parent = None\n",
    "            self.parent_id = None\n",
    "        else:\n",
    "            self.parent = parent_node\n",
    "            self.parent_id = parent_node.cluster_id\n",
    "    \n",
    "    def set_depth(self,depth):\n",
    "        self.depth = depth\n",
    "        \n",
    "    def retrained(self,test_set):\n",
    "        self.last_retrained = test_set\n",
    "    \n",
    "    def certify(self,test_set):\n",
    "        self.last_certified = test_set\n",
    "    \n",
    "    def set_size(self,size):\n",
    "        self.size = size\n",
    "        \n",
    "    def set_cluster_id(self,cluster_id):\n",
    "        self.cluster_id = cluster_id\n",
    "        \n",
    "    def set_data_points(self,data_points):\n",
    "        self.data_points = data_points\n",
    "    \n",
    "    def set_test_labels(self,test_labels,test_set):\n",
    "        if test_set not in self.test_labels.keys():\n",
    "            self.test_labels[test_set] = []\n",
    "        self.test_labels[test_set] = test_labels\n",
    "        \n",
    "    def add_test_points(self,test_point,test_set):\n",
    "        if test_set not in self.test_points.keys():\n",
    "            self.test_points[test_set] = []\n",
    "        self.test_points[test_set].append(test_point)\n",
    "        \n",
    "    def add_predicted(self,predicted,test_set):\n",
    "        if test_set not in self.predicted.keys():\n",
    "            self.predicted[test_set] = []\n",
    "        self.predicted[test_set].append(predicted)\n",
    "    \n",
    "    def set_centroid(self,centroid):\n",
    "        self.centroid = centroid\n",
    "        \n",
    "    def set_classifier(self,classifier):\n",
    "        self.classifier = classifier\n",
    "        \n",
    "    def set_outlier_model(self,outlier_model):\n",
    "        self.outlier_model = outlier_model\n",
    "        \n",
    "    def set_cluster_obj(self,cluster_obj):\n",
    "        self.cluster_obj = cluster_obj\n",
    "        \n",
    "    def add_outlier_points(self,outlier_points):\n",
    "        self.outlier_points.append(outlier_points)\n",
    "    \n",
    "    def reset_outlier_bucket(self):\n",
    "        self.outlier_points = []\n",
    "        \n",
    "    def set_score(self,score):\n",
    "        self.score = score\n",
    "    \n",
    "    def get_score(self):\n",
    "        return self.score\n",
    "        \n",
    "    def add_d1(self,d1):\n",
    "        self.d1 = d1\n",
    "        \n",
    "    def add_d2(self,d2):\n",
    "        self.d2 = d2\n",
    "        \n",
    "    def calculate_threshold(self,outlier_threshold):\n",
    "        self.threshold = max(self.d1,self.d2)*outlier_threshold\n",
    "        \n",
    "    def check_outlier(self,distance):\n",
    "        if self.threshold < distance:\n",
    "            result = True\n",
    "        else:\n",
    "            result = False\n",
    "        return result\n",
    "    \n",
    "    def check_OCS_outlier(self,test_data):\n",
    "        if self.outlier_model.predict([test_data]) == -1:\n",
    "            result = True\n",
    "        else:\n",
    "            result = False\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class birch(object):\n",
    "\n",
    "    def __init__(self,threshold=0.7,branching_factor=40,n_clusters=None,outlier_threshold=0.7):\n",
    "        self.threshold = threshold\n",
    "        self.branching_factor = branching_factor\n",
    "        self.n_clusters = n_clusters\n",
    "        self.outlier_threshold = outlier_threshold\n",
    "        self.Birch_clusterer = Birch(threshold=self.threshold, branching_factor=self.branching_factor,\n",
    "                                     n_clusters=self.n_clusters,compute_sample_indices=True)\n",
    "        self.test_set = 0\n",
    "        self.test_set_X = {}\n",
    "        self.test_set_y = {}\n",
    "    # Fitting the model with train_X\n",
    "    def fit(self,data,y):\n",
    "        self.data = data\n",
    "        print(self.data.shape)\n",
    "        self.y = y\n",
    "        #self.data.drop(self.data.columns[len(self.data.columns)-1], axis=1, inplace=True)\n",
    "        self.Birch_clusterer.fit(self.data)\n",
    "        \n",
    "    def set_test(self,data,y):\n",
    "        self.test_set += 1\n",
    "        self.test_set_X[self.test_set] = data\n",
    "        self.test_set_y[self.test_set] = y\n",
    "\n",
    "    #Defines and builds the Cluster Feature Tree\n",
    "    def get_cluster_tree(self):\n",
    "        self.htree, n_clusters = birch_hierarchy_wrapper(self.Birch_clusterer)\n",
    "        clusters = {}\n",
    "        max_depth = 0\n",
    "        for i in range(n_clusters):\n",
    "            #print('cluster:', i)\n",
    "            node = bcluster()\n",
    "            sub_cluster = self.htree.flatten()[i]\n",
    "            node.set_cluster_id(sub_cluster['cluster_id'])\n",
    "            depth = sub_cluster.current_depth\n",
    "            node.set_depth(depth)\n",
    "            if depth > max_depth:\n",
    "                max_depth = depth\n",
    "            if i not in clusters.keys():\n",
    "                clusters[i] = {}\n",
    "            if sub_cluster.current_depth == 0:\n",
    "                node.set_parent()\n",
    "            else:\n",
    "                node.set_parent(clusters[sub_cluster.parent['cluster_id']])\n",
    "            cluster_size = sub_cluster['cluster_size']\n",
    "            node.set_size(cluster_size)\n",
    "            data_points = sub_cluster['document_id_accumulated']\n",
    "            node.set_data_points(data_points)\n",
    "            centroid = self.data.iloc[sub_cluster['document_id_accumulated'], :].mean(axis=0).values\n",
    "            node.set_centroid(centroid)\n",
    "            d1,d1_v = self.calculate_d1(centroid,data_points)\n",
    "            d2 = self.calculate_d2(centroid,data_points,d1_v)\n",
    "            node.add_d1(d1)\n",
    "            node.add_d2(d2)\n",
    "            node.calculate_threshold(self.outlier_threshold)\n",
    "            clusters[i] = node\n",
    "        return clusters,max_depth\n",
    "    \n",
    "    #Calculate the d1 distance(point farthest away from centroid)\n",
    "    def calculate_d1(self,centroid,data_points):\n",
    "        d1 = 0\n",
    "        u = centroid\n",
    "        d1_v = None\n",
    "        for point in data_points:\n",
    "            v = point\n",
    "            distance = euclidean(u,v)\n",
    "            if distance>d1:\n",
    "                d1 = distance\n",
    "                d1_v = v\n",
    "        return d1,d1_v\n",
    "    \n",
    "    #Calculate the d2 distance(point farthest away from d1 and its distance from centroid)\n",
    "    def calculate_d2(self,centroid,data_points,d1_v):\n",
    "        d2_d1 = 0\n",
    "        u = d1_v\n",
    "        d2_v = None\n",
    "        for point in data_points:\n",
    "            v = point\n",
    "            distance = euclidean(u,v)\n",
    "            if distance>d2_d1:\n",
    "                d2_d1 = distance\n",
    "                d2_v = v\n",
    "        d2 = euclidean(centroid,v)\n",
    "        return d2\n",
    "    \n",
    "    # Display's the tree\n",
    "    def show_clutser_tree(self):\n",
    "        self.htree.display_tree()\n",
    "        \n",
    "    # Add classification model at each node and leaf\n",
    "    def model_adder(self,cluster_tree):\n",
    "        for cluster_id in cluster_tree:\n",
    "            clf = DecisionTreeClassifier(criterion='entropy')\n",
    "            sample_points = cluster_tree[cluster_id].data_points\n",
    "            train_X_sub = self.data.iloc[sample_points,:]\n",
    "            train_y_sub = self.y.iloc[sample_points]\n",
    "            clf.fit(train_X_sub,train_y_sub)\n",
    "            cluster_tree[cluster_id].set_classifier(clf)\n",
    "        return cluster_tree\n",
    "    \n",
    "    def update_model(self,cluster_tree,cluster_id):\n",
    "        clf = DecisionTreeClassifier(criterion='entropy')\n",
    "        sample_points = cluster_tree[cluster_id].data_points\n",
    "        last_retrained = cluster_tree[cluster_id].last_retrained\n",
    "        train_X_sub = self.data.iloc[sample_points,:]\n",
    "        train_y_sub = self.y.iloc[sample_points]\n",
    "        retraining_datasets = cluster_tree[cluster_id].test_points.keys()\n",
    "        for test_set in retraining_datasets:\n",
    "            sample_test_points = cluster_tree[cluster_id].test_points[test_set]\n",
    "            test_X_sub = self.test_set_X[test_set].iloc[sample_test_points,:]\n",
    "            test_y_sub = self.test_set_y[test_set].iloc[sample_test_points]\n",
    "            train_X_sub = pd.concat([train_X_sub,test_X_sub])\n",
    "            train_y_sub = pd.concat([train_y_sub,test_y_sub])\n",
    "        X = train_X_sub\n",
    "        y = train_y_sub\n",
    "        clf.fit(X,y)\n",
    "        cluster_tree[cluster_id].retrained(self.test_set)\n",
    "        cluster_tree[cluster_id].set_classifier(clf)\n",
    "    \n",
    "    def outlier_model_adder(self,cluster_tree):\n",
    "        for cluster_id in cluster_tree:\n",
    "            clf = OneClassSVM(kernel = 'poly',degree = 5,gamma = 'scale',nu=0.4)\n",
    "            sample_points = cluster_tree[cluster_id].data_points\n",
    "            train_X_sub = self.data.iloc[sample_points,:]\n",
    "            clf.fit(train_X_sub)\n",
    "            cluster_tree[cluster_id].set_outlier_model(clf)\n",
    "        return cluster_tree\n",
    "        \n",
    "    # Prediction Function with height based prediction with outlier detection\n",
    "    def predict(self,test_X,depth,do_predict=True):\n",
    "        predicted = []\n",
    "        for test_instance in test_X.iterrows():\n",
    "            test_sample = test_instance[1].values\n",
    "            min_distance = float('inf')\n",
    "            selected_cluster = None\n",
    "            for cluster_id in cluster_tree:\n",
    "                if cluster_tree[cluster_id].depth != depth:\n",
    "                    continue\n",
    "                u = cluster_tree[cluster_id].centroid\n",
    "                v = np.asarray(test_sample,dtype='float64')\n",
    "                distance = euclidean(u,v)\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    selected_cluster = cluster_id\n",
    "            cluster_tree[selected_cluster].add_test_points(test_instance[0])\n",
    "            # Outlier identifier\n",
    "            if cluster_tree[selected_cluster].check_outlier(min_distance):\n",
    "                cluster_tree[selected_cluster].add_outlier_points(test_instance[0])\n",
    "            if do_predict:\n",
    "                _predicted_label = cluster_tree[selected_cluster].classifier.predict([test_sample])\n",
    "                cluster_tree[selected_cluster].add_predicted(_predicted_label)\n",
    "                predicted.append(_predicted_label)\n",
    "        return predicted\n",
    "    \n",
    "    def distance(self,x,y):\n",
    "        dist = (list(x[:,1]) - y)**2\n",
    "        dist = np.sum(dist, axis=1)\n",
    "        dist = np.sqrt(dist)\n",
    "        ind = np.unravel_index(np.argmin(dist, axis=None), dist.shape)\n",
    "        min_distance = dist[np.argmin(dist, axis=None)]\n",
    "        return list(x[ind])[0],min_distance\n",
    "    \n",
    "    # New Predict\n",
    "    def predict_new(self,test_X,depth,cluster_tree,do_predict=True):\n",
    "        predicted = []\n",
    "        cluster_centroids = []\n",
    "        for cluster_id in cluster_tree:\n",
    "            cluster_tree[cluster_id].reset_outlier_bucket()\n",
    "            if cluster_tree[cluster_id].depth != depth:\n",
    "                continue\n",
    "            cluster_centroids.append([cluster_id,cluster_tree[cluster_id].centroid])\n",
    "        cluster_centroids = np.array(cluster_centroids)\n",
    "        for test_instance in test_X.iterrows():\n",
    "            test_sample = np.array(test_instance[1].values)\n",
    "            selected_cluster,min_distance = self.distance(cluster_centroids,test_sample)\n",
    "            cluster_tree[selected_cluster].add_test_points(test_instance[0],self.test_set)\n",
    "            # Outlier identifier\n",
    "            #if cluster_tree[selected_cluster].check_outlier(min_distance):\n",
    "            #    cluster_tree[selected_cluster].add_outlier_points(test_instance[0])\n",
    "            if cluster_tree[selected_cluster].check_OCS_outlier(test_sample):\n",
    "                cluster_tree[selected_cluster].add_outlier_points(test_instance[0])\n",
    "            if do_predict:\n",
    "                _predicted_label = cluster_tree[selected_cluster].classifier.predict([test_sample])\n",
    "                cluster_tree[selected_cluster].add_predicted(_predicted_label[0],self.test_set)\n",
    "                predicted.append(_predicted_label[0])\n",
    "        return predicted\n",
    "    \n",
    "    # Model certification creator\n",
    "    def certify_model(self,cluster_tree,test_y):\n",
    "        for cluster_id in cluster_tree:\n",
    "            if len(cluster_tree[cluster_id].test_points.keys()) == 0:\n",
    "                continue\n",
    "            if self.test_set not in cluster_tree[cluster_id].test_points.keys():\n",
    "                continue\n",
    "            cluster_tree[cluster_id].set_test_labels(test_y[cluster_tree[cluster_id].test_points[self.test_set]].values,self.test_set)\n",
    "            precision = metrics.precision_score(cluster_tree[cluster_id].test_labels[self.test_set], \n",
    "                                                cluster_tree[cluster_id].predicted[self.test_set],average='weighted')\n",
    "            recall = metrics.recall_score(cluster_tree[cluster_id].test_labels[self.test_set], \n",
    "                                          cluster_tree[cluster_id].predicted[self.test_set],average='weighted')\n",
    "            f1_Score = metrics.f1_score(cluster_tree[cluster_id].test_labels[self.test_set], \n",
    "                                        cluster_tree[cluster_id].predicted[self.test_set],average='weighted')\n",
    "            score = {'precision': precision,'recall': recall,'f1_Score': f1_Score}\n",
    "            cluster_tree[cluster_id].set_score(score)\n",
    "            \n",
    "            \n",
    "    def check_model(self,cluster_tree,threshold = 0.7):\n",
    "        score = {}\n",
    "        for cluster_id in cluster_tree:\n",
    "            if len(cluster_tree[cluster_id].test_points) == 0:\n",
    "                continue\n",
    "            score[cluster_id] =  cluster_tree[cluster_id].get_score()\n",
    "            if score[cluster_id]['f1_Score'] < threshold:\n",
    "                print('retreining',score[cluster_id]['f1_Score'])\n",
    "                self.update_model(cluster_tree,cluster_id)\n",
    "                \n",
    "    def rebuild_models(self,cluster):\n",
    "        train_X_sub = self.data\n",
    "        train_y_sub = self.y\n",
    "        for test_set in range(1,self.test_set+1):\n",
    "            test_X_sub = self.test_set_X[test_set]\n",
    "            test_y_sub = self.test_set_y[test_set]\n",
    "            train_X_sub = pd.concat([train_X_sub,test_X_sub])\n",
    "            train_y_sub = pd.concat([train_y_sub,test_y_sub])\n",
    "        X = train_X_sub.reset_index(drop=True)\n",
    "        y = train_y_sub.reset_index(drop=True)\n",
    "        self.fit(X,y)\n",
    "        self.test_set = 0\n",
    "        self.test_set_X = {}\n",
    "        self.test_set_y = {}\n",
    "        cluster_tree,max_depth = cluster.get_cluster_tree()\n",
    "        print(cluster)\n",
    "        cluster_tree = cluster.model_adder(cluster_tree)\n",
    "        cluster_tree = cluster.outlier_model_adder(cluster_tree)\n",
    "        return cluster_tree,max_depth\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path,target):\n",
    "    df = pd.read_csv(path)\n",
    "    y = df[target]\n",
    "    X = df.drop(labels = target, axis = 1)\n",
    "    X = X.apply(pd.to_numeric)\n",
    "    return df,X,y\n",
    "\n",
    "def get_data(file_path,target='defects',normal_class='normal'):\n",
    "    train_df, train_X, train_y = load_data(file,target)\n",
    "    y_train = []\n",
    "    for instance in train_y.values:\n",
    "        if instance == normal_class:\n",
    "            y_train.append(1)\n",
    "        else:\n",
    "            y_train.append(-1)\n",
    "    y_train = pd.Series(y_train)\n",
    "    train_df.defects.unique()\n",
    "    return train_df,train_X,y_train\n",
    "\n",
    "# Cluster Driver\n",
    "def cluster_driver(file,print_tree = False):\n",
    "    train_df, train_X, train_y = get_data(file)\n",
    "    cluster = birch(branching_factor=20)\n",
    "    cluster.fit(train_X,train_y)\n",
    "    cluster_tree,max_depth = cluster.get_cluster_tree()\n",
    "    cluster_tree = cluster.model_adder(cluster_tree)\n",
    "    cluster_tree = cluster.outlier_model_adder(cluster_tree)\n",
    "    if print_tree:\n",
    "        cluster.show_clutser_tree()\n",
    "    return cluster,cluster_tree,max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00       583\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           1.00       583\n",
      "   macro avg       0.50      0.50      0.50       583\n",
      "weighted avg       1.00      1.00      1.00       583\n",
      "\n",
      "19 206\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00       552\n",
      "\n",
      "    accuracy                           1.00       552\n",
      "   macro avg       1.00      1.00      1.00       552\n",
      "weighted avg       1.00      1.00      1.00       552\n",
      "\n",
      "42 201\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00       602\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           1.00       602\n",
      "   macro avg       0.50      0.50      0.50       602\n",
      "weighted avg       1.00      1.00      1.00       602\n",
      "\n",
      "58 230\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00       530\n",
      "\n",
      "    accuracy                           1.00       530\n",
      "   macro avg       1.00      1.00      1.00       530\n",
      "weighted avg       1.00      1.00      1.00       530\n",
      "\n",
      "62 163\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00       538\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           1.00       538\n",
      "   macro avg       0.50      0.50      0.50       538\n",
      "weighted avg       1.00      1.00      1.00       538\n",
      "\n",
      "73 211\n",
      "======================================================\n",
      "retreining 0.0\n",
      "retreining 0.0\n",
      "retreining 0.0\n",
      "retreining 0.0\n",
      "retreining 0.0\n",
      "retreining 0.7142857142857143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00       582\n",
      "\n",
      "    accuracy                           1.00       582\n",
      "   macro avg       1.00      1.00      1.00       582\n",
      "weighted avg       1.00      1.00      1.00       582\n",
      "\n",
      "73 174\n",
      "======================================================\n",
      "retreining 0.0\n",
      "retreining 0.0\n",
      "retreining 0.0\n",
      "retreining 0.0\n",
      "retreining 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00       554\n",
      "\n",
      "    accuracy                           1.00       554\n",
      "   macro avg       1.00      1.00      1.00       554\n",
      "weighted avg       1.00      1.00      1.00       554\n",
      "\n",
      "73 229\n",
      "======================================================\n",
      "retreining 0.0\n",
      "retreining 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00       604\n",
      "\n",
      "    accuracy                           1.00       604\n",
      "   macro avg       1.00      1.00      1.00       604\n",
      "weighted avg       1.00      1.00      1.00       604\n",
      "\n",
      "73 270\n",
      "======================================================\n",
      "retreining 0.0\n",
      "retreining 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00       533\n",
      "\n",
      "    accuracy                           1.00       533\n",
      "   macro avg       1.00      1.00      1.00       533\n",
      "weighted avg       1.00      1.00      1.00       533\n",
      "\n",
      "73 141\n",
      "======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00       546\n",
      "\n",
      "    accuracy                           1.00       546\n",
      "   macro avg       1.00      1.00      1.00       546\n",
      "weighted avg       1.00      1.00      1.00       546\n",
      "\n",
      "73 235\n"
     ]
    }
   ],
   "source": [
    "# getting the cluster tree\n",
    "total_df = pd.DataFrame()\n",
    "for i in range(10):\n",
    "    predicted = {}\n",
    "    file_normal = 'Data/NSL-KDD/modified/Train/Normal_Data/N' + str(i) + '.csv'\n",
    "    file_anomaly = 'Data/NSL-KDD/modified/Train/5_anomaly/A' + str(i) + '.csv'\n",
    "    file_mixed = 'Data/NSL-KDD/modified/Train/Mixed_data/M' + str(i) + '.csv'\n",
    "    if i == 11:\n",
    "        normal_df,_,_ = load_data(file_normal,'defects')\n",
    "        cluster,cluster_tree,max_depth = cluster_driver(file_normal)\n",
    "    if i < 5:\n",
    "        normal_df,_,_ = load_data(file_normal,'defects')\n",
    "        anomaly_df,_,_ = load_data(file_anomaly,'defects')\n",
    "        total_df = pd.concat([total_df,normal_df])\n",
    "        total_df = pd.concat([total_df,anomaly_df])\n",
    "        _\n",
    "        total_df.to_csv('Data/NSL-KDD/modified/Train/mixed_Data/M' + str(i+5) + '.csv',index=False)\n",
    "        #cluster,cluster_tree,max_depth = cluster_driver(file_normal)\n",
    "    else:\n",
    "        print(\"======================================================\")\n",
    "        #cluster,cluster_tree,max_depth = cluster_driver(file_mixed)\n",
    "    test_df,test_X,test_y = get_data(file_anomaly)\n",
    "    cluster.set_test(test_X,test_y)\n",
    "    for depth in range(max_depth):\n",
    "        predicted[depth] = cluster.predict_new(test_X,depth,cluster_tree,True)\n",
    "    cluster.certify_model(cluster_tree,test_y)\n",
    "    if i >= 5:\n",
    "        cluster.check_model(cluster_tree,threshold=0.8)\n",
    "    print(metrics.classification_report(test_y, predicted[1]))\n",
    "    total = 0\n",
    "    j = 0\n",
    "    for i in cluster_tree:\n",
    "        if len(cluster_tree[i].test_points) == 0:\n",
    "            continue\n",
    "        #print(\"Percentage Identified\",len(cluster_tree[i].data_points),len(cluster_tree[i].test_points),len(cluster_tree[i].outlier_points)/len(cluster_tree[i].test_points))\n",
    "        j += 1\n",
    "        total += len(cluster_tree[i].outlier_points)\n",
    "    print(j,total)\n",
    "    #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7439, 119)\n",
      "<__main__.birch object at 0x1a18e21b38>\n"
     ]
    }
   ],
   "source": [
    "cluster_tree,max_depth = cluster.rebuild_models(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check for outlies detected\n",
    "total = 0\n",
    "j = 0\n",
    "for i in cluster_tree:\n",
    "    if len(cluster_tree[i].test_points) == 0:\n",
    "        continue\n",
    "    print(\"Percentage Identified\",len(cluster_tree[i].data_points),len(cluster_tree[i].test_points),len(cluster_tree[i].outlier_points)/len(cluster_tree[i].test_points))\n",
    "    j += 1\n",
    "    total += len(cluster_tree[i].outlier_points)\n",
    "print(j,total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://yuml.me/64d2bf81.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
