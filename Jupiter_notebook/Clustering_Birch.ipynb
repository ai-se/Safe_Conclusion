{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from graphviz import Digraph\n",
    "import scipy.spatial.distance\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "#Clustering birch\n",
    "from freediscovery.cluster import birch_hierarchy_wrapper\n",
    "from freediscovery.cluster import Birch,BirchSubcluster\n",
    "#Sklearn\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "#Learners\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#Distance measure\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class birch(object):\n",
    "\n",
    "    def __init__(self,threshold=0.7,branching_factor=20,n_clusters=None):\n",
    "        self.threshold = threshold\n",
    "        self.branching_factor = branching_factor\n",
    "        self.n_clusters = n_clusters\n",
    "        self.Birch_clusterer = Birch(threshold=self.threshold, branching_factor=self.branching_factor,\n",
    "                                     n_clusters=self.n_clusters,compute_sample_indices=True)\n",
    "    \n",
    "    def fit(self,data,y):\n",
    "        self.data = data\n",
    "        self.y = y\n",
    "        #self.data.drop(self.data.columns[len(self.data.columns)-1], axis=1, inplace=True)\n",
    "        self.Birch_clusterer.fit(self.data)\n",
    "\n",
    "    def get_cluster_tree(self):\n",
    "        self.htree, n_clusters = birch_hierarchy_wrapper(self.Birch_clusterer)\n",
    "        clusters = {}\n",
    "        max_depth = 0\n",
    "        for i in range(n_clusters):\n",
    "            sub_cluster = self.htree.flatten()[i]\n",
    "            depth = sub_cluster.current_depth\n",
    "            if depth > max_depth:\n",
    "                max_depth = depth\n",
    "            if depth not in clusters.keys():\n",
    "                clusters[depth] = {}\n",
    "            if i not in clusters[depth].keys():\n",
    "                clusters[depth][i] = {}\n",
    "            if sub_cluster.current_depth == 0:\n",
    "                clusters[depth][i]['parent'] = None\n",
    "            else:\n",
    "                clusters[depth][i]['parent'] = sub_cluster.parent['cluster_id']\n",
    "            clusters[depth][i]['depth'] = sub_cluster.current_depth\n",
    "            clusters[depth][i]['size'] = sub_cluster['cluster_size']\n",
    "            clusters[depth][i]['data_points'] = sub_cluster['document_id_accumulated']\n",
    "            clusters[depth][i]['centroid'] = self.data.iloc[sub_cluster['document_id_accumulated'], :].mean(axis=0).values\n",
    "        return clusters,max_depth\n",
    "    \n",
    "    def show_clutser_tree(self):\n",
    "        self.htree.display_tree()\n",
    "        \n",
    "    def model_adder(self,cluster_tree):\n",
    "        for depth in cluster_tree:\n",
    "            for cluster_id in cluster_tree[depth]:\n",
    "                clf = DecisionTreeClassifier(criterion='entropy')\n",
    "                sample_points = cluster_tree[depth][cluster_id]['data_points']\n",
    "                train_X_sub = self.data.iloc[sample_points,:]\n",
    "                train_y_sub = self.y.iloc[sample_points]\n",
    "                clf.fit(train_X_sub,train_y_sub)\n",
    "                cluster_tree[depth][cluster_id]['clf'] = clf\n",
    "        return cluster_tree\n",
    "        \n",
    "    def predict(self,data,depth):\n",
    "        depth = 0\n",
    "        predicted = []\n",
    "        for i in range(test_X.shape[0]):\n",
    "            test_sample = test_X.iloc[i].tolist()\n",
    "            min_distance = float('inf')\n",
    "            selected_cluster = None\n",
    "            for cluster_id in cluster_tree[depth]:\n",
    "                u = cluster_tree[depth][cluster_id]['centroid']\n",
    "                v = np.asarray(test_sample,dtype='float64')\n",
    "                distance = euclidean(u,v)\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    selected_cluster = cluster_id\n",
    "            predicted.append(cluster_tree[depth][selected_cluster]['clf'].predict([test_sample]))\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path,target):\n",
    "    df = pd.read_csv(path)\n",
    "    if path == 'data/jm1.csv':\n",
    "        df = df[~df.uniq_Op.str.contains(\"\\?\")]\n",
    "    y = df[target]\n",
    "    X = df.drop(labels = target, axis = 1)\n",
    "    X = X.apply(pd.to_numeric)\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    return train_X, test_X, train_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster Driver\n",
    "def cluster_driver(file,print_tree = True):\n",
    "    train_X, test_X, train_y, test_y = load_data(file,'defects')\n",
    "    cluster = birch(branching_factor=20)\n",
    "    cluster.fit(train_X,train_y)\n",
    "    cluster_tree,max_depth = cluster.get_cluster_tree()\n",
    "    cluster_tree = cluster.model_adder(cluster_tree)\n",
    "    if print_tree:\n",
    "        cluster.show_clutser_tree()\n",
    "    return cluster,cluster_tree,max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cluster_id=0] N_children: 11 N_samples: 667\n",
      "> [cluster_id=1] N_children: 0 N_samples: 8\n",
      "> [cluster_id=2] N_children: 3 N_samples: 27\n",
      "> > [cluster_id=3] N_children: 0 N_samples: 6\n",
      "> > [cluster_id=4] N_children: 0 N_samples: 4\n",
      "> > [cluster_id=5] N_children: 0 N_samples: 17\n",
      "> [cluster_id=6] N_children: 3 N_samples: 44\n",
      "> > [cluster_id=7] N_children: 0 N_samples: 13\n",
      "> > [cluster_id=8] N_children: 0 N_samples: 20\n",
      "> > [cluster_id=9] N_children: 0 N_samples: 11\n",
      "> [cluster_id=10] N_children: 9 N_samples: 100\n",
      "> > [cluster_id=11] N_children: 0 N_samples: 13\n",
      "> > [cluster_id=12] N_children: 0 N_samples: 8\n",
      "> > [cluster_id=13] N_children: 0 N_samples: 20\n",
      "> > [cluster_id=14] N_children: 0 N_samples: 16\n",
      "> > [cluster_id=15] N_children: 0 N_samples: 18\n",
      "> > [cluster_id=16] N_children: 0 N_samples: 16\n",
      "> > [cluster_id=17] N_children: 0 N_samples: 1\n",
      "> > [cluster_id=18] N_children: 0 N_samples: 1\n",
      "> > [cluster_id=19] N_children: 0 N_samples: 7\n",
      "> [cluster_id=20] N_children: 0 N_samples: 2\n",
      "> [cluster_id=21] N_children: 7 N_samples: 57\n",
      "> > [cluster_id=22] N_children: 0 N_samples: 9\n",
      "> > [cluster_id=23] N_children: 0 N_samples: 1\n",
      "> > [cluster_id=24] N_children: 0 N_samples: 11\n",
      "> > [cluster_id=25] N_children: 0 N_samples: 5\n",
      "> > [cluster_id=26] N_children: 0 N_samples: 6\n",
      "> > [cluster_id=27] N_children: 0 N_samples: 15\n",
      "> > [cluster_id=28] N_children: 0 N_samples: 10\n",
      "> [cluster_id=29] N_children: 17 N_samples: 172\n",
      "> > [cluster_id=30] N_children: 0 N_samples: 19\n",
      "> > [cluster_id=31] N_children: 0 N_samples: 5\n",
      "> > [cluster_id=32] N_children: 0 N_samples: 7\n",
      "> > [cluster_id=33] N_children: 0 N_samples: 15\n",
      "> > [cluster_id=34] N_children: 0 N_samples: 7\n",
      "> > [cluster_id=35] N_children: 0 N_samples: 12\n",
      "> > [cluster_id=36] N_children: 0 N_samples: 16\n",
      "> > [cluster_id=37] N_children: 0 N_samples: 12\n",
      "> > [cluster_id=38] N_children: 0 N_samples: 1\n",
      "> > [cluster_id=39] N_children: 0 N_samples: 9\n",
      "> > [cluster_id=40] N_children: 0 N_samples: 2\n",
      "> > [cluster_id=41] N_children: 0 N_samples: 20\n",
      "> > [cluster_id=42] N_children: 0 N_samples: 17\n",
      "> > [cluster_id=43] N_children: 0 N_samples: 11\n",
      "> > [cluster_id=44] N_children: 0 N_samples: 10\n",
      "> > [cluster_id=45] N_children: 0 N_samples: 3\n",
      "> > [cluster_id=46] N_children: 0 N_samples: 6\n",
      "> [cluster_id=47] N_children: 2 N_samples: 19\n",
      "> > [cluster_id=48] N_children: 0 N_samples: 16\n",
      "> > [cluster_id=49] N_children: 0 N_samples: 3\n",
      "> [cluster_id=50] N_children: 3 N_samples: 32\n",
      "> > [cluster_id=51] N_children: 0 N_samples: 6\n",
      "> > [cluster_id=52] N_children: 0 N_samples: 11\n",
      "> > [cluster_id=53] N_children: 0 N_samples: 15\n",
      "> [cluster_id=54] N_children: 2 N_samples: 27\n",
      "> > [cluster_id=55] N_children: 0 N_samples: 18\n",
      "> > [cluster_id=56] N_children: 0 N_samples: 9\n",
      "> [cluster_id=57] N_children: 18 N_samples: 179\n",
      "> > [cluster_id=58] N_children: 0 N_samples: 18\n",
      "> > [cluster_id=59] N_children: 0 N_samples: 2\n",
      "> > [cluster_id=60] N_children: 0 N_samples: 2\n",
      "> > [cluster_id=61] N_children: 0 N_samples: 9\n",
      "> > [cluster_id=62] N_children: 0 N_samples: 20\n",
      "> > [cluster_id=63] N_children: 0 N_samples: 9\n",
      "> > [cluster_id=64] N_children: 0 N_samples: 15\n",
      "> > [cluster_id=65] N_children: 0 N_samples: 19\n",
      "> > [cluster_id=66] N_children: 0 N_samples: 18\n",
      "> > [cluster_id=67] N_children: 0 N_samples: 1\n",
      "> > [cluster_id=68] N_children: 0 N_samples: 9\n",
      "> > [cluster_id=69] N_children: 0 N_samples: 4\n",
      "> > [cluster_id=70] N_children: 0 N_samples: 14\n",
      "> > [cluster_id=71] N_children: 0 N_samples: 6\n",
      "> > [cluster_id=72] N_children: 0 N_samples: 17\n",
      "> > [cluster_id=73] N_children: 0 N_samples: 6\n",
      "> > [cluster_id=74] N_children: 0 N_samples: 3\n",
      "> > [cluster_id=75] N_children: 0 N_samples: 7\n"
     ]
    }
   ],
   "source": [
    "# getting the cluster tree\n",
    "file = 'data/JDT.csv'\n",
    "cluster,cluster_tree,max_depth = cluster_driver(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.86      0.88      0.87       258\n",
      "        True       0.53      0.49      0.51        72\n",
      "\n",
      "   micro avg       0.79      0.79      0.79       330\n",
      "   macro avg       0.70      0.68      0.69       330\n",
      "weighted avg       0.79      0.79      0.79       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Base performance score\n",
    "clf = DecisionTreeClassifier(criterion='entropy')\n",
    "train_X, test_X, train_y, test_y = load_data(file,'defects')\n",
    "clf.fit(train_X, train_y)\n",
    "predicted = clf.predict(test_X)\n",
    "print(metrics.classification_report(test_y, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mutated_data(path,target):\n",
    "    train_X, test_X, train_y, test_y = load_data(path,target)\n",
    "    test_X = pd.concat([train_X,test_X])\n",
    "    test_y = pd.concat([train_y,test_y])\n",
    "    return test_X,test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.54      0.03      0.06       213\n",
      "        True       0.35      0.95      0.51       117\n",
      "\n",
      "   micro avg       0.36      0.36      0.36       330\n",
      "   macro avg       0.44      0.49      0.29       330\n",
      "weighted avg       0.47      0.36      0.22       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Birch classifier score(mention depth)\n",
    "file = 'data/JDT_19.csv'\n",
    "test_X,test_y = load_mutated_data(file,'defects')\n",
    "depth = max_depth\n",
    "predicted = cluster.predict(test_X,depth)\n",
    "print(metrics.classification_report(test_y, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
